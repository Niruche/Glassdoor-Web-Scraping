{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glassdoor Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\nirun\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nirun\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "C:\\Users\\nirun\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\fuzzywuzzy\\fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#numerical library for python\n",
    "import numpy as np\n",
    "\n",
    "#headless browser\n",
    "# go to anaconda prompt and enter: pip install selenium\n",
    "from selenium import webdriver\n",
    "\n",
    "from os import path\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "\n",
    "#make pc not do anything for certain amount of time\n",
    "from time import sleep\n",
    "\n",
    "import re\n",
    "\n",
    "# Load library\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "#datetime package\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# fuzz is used to compare TWO strings\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "# process is used to compare a string to MULTIPLE other strings\n",
    "from fuzzywuzzy import process\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 703 entries, 0 to 702\n",
      "Data columns (total 20 columns):\n",
      "date                         703 non-null datetime64[ns]\n",
      "star                         703 non-null int64\n",
      "subject                      702 non-null object\n",
      "pros                         703 non-null object\n",
      "cons                         703 non-null object\n",
      "advice                       394 non-null object\n",
      "location                     408 non-null object\n",
      "clean_member_type            702 non-null object\n",
      "tenure                       703 non-null object\n",
      "employee_status              703 non-null object\n",
      "recommend                    520 non-null object\n",
      "outlook                      512 non-null object\n",
      "ceo                          505 non-null object\n",
      "Career Opportunities         621 non-null object\n",
      "Compensation and Benefits    620 non-null object\n",
      "Culture & Values             609 non-null object\n",
      "Senior Management            601 non-null object\n",
      "Work/Life Balance            621 non-null object\n",
      "star_group                   703 non-null object\n",
      "unique_id                    703 non-null int64\n",
      "dtypes: datetime64[ns](1), int64(2), object(17)\n",
      "memory usage: 109.9+ KB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('glassdoor_full_dataset.csv')\n",
    "df['date'] = pd.to_datetime(df.date)\n",
    "df['unique_id'] = df.index\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New columns with symbols removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sw_pros'] = df.pros.fillna('none')\n",
    "df['sw_pros'] = df['sw_pros'].map(lambda x: re.sub('[^a-zA-Z]', ' ', x))\n",
    "df['sw_pros'] =df['sw_pros'].map(lambda x: re.sub(r'\\s+', ' ', x))\n",
    "\n",
    "df['sw_cons'] = df.cons.fillna('none')\n",
    "df['sw_cons'] = df['sw_cons'].map(lambda x: re.sub('[^a-zA-Z]', ' ', x))\n",
    "df['sw_cons'] =df['sw_cons'].map(lambda x: re.sub(r'\\s+', ' ', x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New columns with stopwords removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>star</th>\n",
       "      <th>subject</th>\n",
       "      <th>pros</th>\n",
       "      <th>cons</th>\n",
       "      <th>advice</th>\n",
       "      <th>location</th>\n",
       "      <th>clean_member_type</th>\n",
       "      <th>tenure</th>\n",
       "      <th>employee_status</th>\n",
       "      <th>...</th>\n",
       "      <th>ceo</th>\n",
       "      <th>Career Opportunities</th>\n",
       "      <th>Compensation and Benefits</th>\n",
       "      <th>Culture &amp; Values</th>\n",
       "      <th>Senior Management</th>\n",
       "      <th>Work/Life Balance</th>\n",
       "      <th>star_group</th>\n",
       "      <th>unique_id</th>\n",
       "      <th>sw_pros</th>\n",
       "      <th>sw_cons</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-11-04</td>\n",
       "      <td>5</td>\n",
       "      <td>Big dynamic company with many opportunities</td>\n",
       "      <td>Fast grows up if you are hard worker</td>\n",
       "      <td>unstable rota and early morning shifts</td>\n",
       "      <td>NaN</td>\n",
       "      <td>London</td>\n",
       "      <td>Team Leader</td>\n",
       "      <td>I have been working at Pret A Manger full-time...</td>\n",
       "      <td>Current Employee</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Positive</td>\n",
       "      <td>0</td>\n",
       "      <td>fast grows hard worker</td>\n",
       "      <td>unstable rota early morning shifts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-20</td>\n",
       "      <td>5</td>\n",
       "      <td>Great Vacation Work</td>\n",
       "      <td>Lots of shifts and flexibility of hours</td>\n",
       "      <td>Management are lax on food waste</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Gatwick</td>\n",
       "      <td>Team Member</td>\n",
       "      <td>I worked at Pret A Manger full-time for less t...</td>\n",
       "      <td>Former Employee</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "      <td>1</td>\n",
       "      <td>shifts flexibility hours</td>\n",
       "      <td>management lax food waste</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-20</td>\n",
       "      <td>4</td>\n",
       "      <td>Great company</td>\n",
       "      <td>Great training, benefits and salary</td>\n",
       "      <td>Nothing to say so far</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Team Leader</td>\n",
       "      <td>I have been working at Pret A Manger full-time</td>\n",
       "      <td>Current Employee</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Positive</td>\n",
       "      <td>2</td>\n",
       "      <td>great training benefits salary</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-16</td>\n",
       "      <td>3</td>\n",
       "      <td>Something Helpful</td>\n",
       "      <td>The worst thing is loss of bonus.</td>\n",
       "      <td>New management created silly rules and people ...</td>\n",
       "      <td>Respect your staff more.They are amazing hard ...</td>\n",
       "      <td>London</td>\n",
       "      <td>Production Team Member</td>\n",
       "      <td>I worked at Pret A Manger full-time for more t...</td>\n",
       "      <td>Former Employee</td>\n",
       "      <td>...</td>\n",
       "      <td>positive</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>3</td>\n",
       "      <td>worst loss bonus</td>\n",
       "      <td>management created silly rules people working ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-15</td>\n",
       "      <td>3</td>\n",
       "      <td>One day probation</td>\n",
       "      <td>Lot to learn, busy, new people</td>\n",
       "      <td>Very busy</td>\n",
       "      <td>Be more patient with newbies</td>\n",
       "      <td>Palmers Green</td>\n",
       "      <td>Kitchen Staff</td>\n",
       "      <td>I worked at Pret A Manger full-time for less t...</td>\n",
       "      <td>Former Employee</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>4</td>\n",
       "      <td>learn busy people</td>\n",
       "      <td>busy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        date  star                                      subject  \\\n",
       "0 2019-11-04     5  Big dynamic company with many opportunities   \n",
       "1 2020-01-20     5                          Great Vacation Work   \n",
       "2 2020-01-20     4                                Great company   \n",
       "3 2020-01-16     3                            Something Helpful   \n",
       "4 2020-01-15     3                            One day probation   \n",
       "\n",
       "                                      pros  \\\n",
       "0     Fast grows up if you are hard worker   \n",
       "1  Lots of shifts and flexibility of hours   \n",
       "2      Great training, benefits and salary   \n",
       "3        The worst thing is loss of bonus.   \n",
       "4           Lot to learn, busy, new people   \n",
       "\n",
       "                                                cons  \\\n",
       "0             unstable rota and early morning shifts   \n",
       "1                   Management are lax on food waste   \n",
       "2                              Nothing to say so far   \n",
       "3  New management created silly rules and people ...   \n",
       "4                                          Very busy   \n",
       "\n",
       "                                              advice        location  \\\n",
       "0                                                NaN          London   \n",
       "1                                                NaN         Gatwick   \n",
       "2                                                NaN             NaN   \n",
       "3  Respect your staff more.They are amazing hard ...          London   \n",
       "4                       Be more patient with newbies   Palmers Green   \n",
       "\n",
       "          clean_member_type  \\\n",
       "0              Team Leader    \n",
       "1              Team Member    \n",
       "2              Team Leader    \n",
       "3   Production Team Member    \n",
       "4            Kitchen Staff    \n",
       "\n",
       "                                              tenure   employee_status  \\\n",
       "0  I have been working at Pret A Manger full-time...  Current Employee   \n",
       "1  I worked at Pret A Manger full-time for less t...   Former Employee   \n",
       "2     I have been working at Pret A Manger full-time  Current Employee   \n",
       "3  I worked at Pret A Manger full-time for more t...   Former Employee   \n",
       "4  I worked at Pret A Manger full-time for less t...   Former Employee   \n",
       "\n",
       "                         ...                               ceo  \\\n",
       "0                        ...                               NaN   \n",
       "1                        ...                               NaN   \n",
       "2                        ...                               NaN   \n",
       "3                        ...                          positive   \n",
       "4                        ...                               NaN   \n",
       "\n",
       "  Career Opportunities Compensation and Benefits Culture & Values  \\\n",
       "0             Positive                  Positive         Positive   \n",
       "1             Positive                  Positive         Positive   \n",
       "2             Positive                  Positive         Positive   \n",
       "3             Negative                  Negative         Negative   \n",
       "4              Neutral                   Neutral          Neutral   \n",
       "\n",
       "  Senior Management Work/Life Balance star_group unique_id  \\\n",
       "0          Positive           Neutral   Positive         0   \n",
       "1          Positive          Positive   Positive         1   \n",
       "2          Positive           Neutral   Positive         2   \n",
       "3           Neutral          Negative    Neutral         3   \n",
       "4           Neutral           Neutral    Neutral         4   \n",
       "\n",
       "                          sw_pros  \\\n",
       "0          fast grows hard worker   \n",
       "1        shifts flexibility hours   \n",
       "2  great training benefits salary   \n",
       "3                worst loss bonus   \n",
       "4               learn busy people   \n",
       "\n",
       "                                             sw_cons  \n",
       "0                 unstable rota early morning shifts  \n",
       "1                          management lax food waste  \n",
       "2                                                     \n",
       "3  management created silly rules people working ...  \n",
       "4                                               busy  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load stop words\n",
    "stop = stopwords.words('english')\n",
    "newStopWords = ['A','also','can', 'Can', 'i', 'dedpends', 'go' ,'too','if','I', 'can', 'it', 'not' , \n",
    "                'nt', 'get', 'you','worked','not','if','lot','per','even','nt','enough','it','always',\n",
    "                'there','need','lots','part','can','bit','last','things','thing','put','make',\"a\", \"about\", \n",
    "                \"above\", \"after\", \"again\", \"against\", \"ain\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"aren\", \n",
    "                \"aren't\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \n",
    "                \"but\", \"by\", \"can\", \"couldn\", \"couldn't\", \"d\", \"did\", \"didn\", \"didn't\", \"do\", \"does\", \"doesn\", \n",
    "                \"doesn't\", \"doing\", \"don\", \"don't\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \n",
    "                \"had\", \"hadn\", \"hadn't\", \"has\", \"hasn\", \"hasn't\", \"have\", \"haven\", \"haven't\", \"having\", \"he\", \n",
    "                \"her\", \"here\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"i\", \"if\", \"in\", \"into\", \"is\", \n",
    "                \"isn\", \"isn't\", \"it\", \"it's\", \"its\", \"itself\", \"just\", \"ll\", \"m\", \"ma\", \"me\", \"mightn\", \"mightn't\", \n",
    "                \"more\", \"most\", \"mustn\", \"mustn't\", \"my\", \"myself\", \"needn\", \"needn't\", \"no\", \"nor\", \"not\", \"now\", \n",
    "                \"o\", \"of\", \"off\", \"on\", \"once\", \"only\", \"or\", \"other\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \n",
    "                \"own\", \"re\", \"s\", \"same\", \"shan\", \"shan't\", \"she\", \"she's\", \"should\", \"should've\", \"shouldn\", \n",
    "                \"shouldn't\", \"so\", \"some\", \"such\", \"t\", \"than\", \"that\", \"that'll\", \"the\", \"their\", \"theirs\", \"them\", \n",
    "                \"themselves\", \"then\", \"there\", \"these\", \"they\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \n",
    "                \"until\", \"up\", \"ve\", \"very\", \"was\", \"wasn\", \"wasn't\", \"we\", \"were\", \"weren\", \"weren't\", \"what\", \"when\", \n",
    "                \"where\", \"which\", \"while\", \"who\", \"whom\", \"why\", \"will\", \"with\", \"won\", \"won't\", \"wouldn\", \"wouldn't\", \n",
    "                \"y\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"could\", \n",
    "                \"he'd\", \"he'll\", \"he's\", \"here's\", \"how's\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"let's\", \"ought\", \"she'd\", \n",
    "                \"she'll\", \"that's\", \"there's\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"we'd\", \"we'll\", \"we're\", \n",
    "                \"we've\", \"what's\", \"when's\", \"where's\", \"who's\", \"why's\", \"would\",'de','la','the', 'day', 'la', 'like',\n",
    "                'customers','really','en','Manger','que','customer','every','many','It','El','one','much','You',\n",
    "                'well','un','Sometimes','never','una','los','give','If','everyone','want','They','There','week',\n",
    "                'Everything','n','empresa','di','first','know','weekly','keep','del','co','Lo','es','il','por','era',\n",
    "                \"a\",\t\"about\",\t\"above\",\t\"after\",\t\"again\",\t\"against\",\t\"all\",\t\"am\",\t\"an\",\t\"and\",\t\"any\",\t\"are\",\t\"aren't\",\t\"as\",\t\"at\",\t\"be\",\t\"because\",\n",
    "                \"been\",\t\"before\",\t\"being\",\t\"below\",\t\"between\",\t\"both\",\t\"but\",\t\"by\",\t\"can't\",\t\"cannot\",\t\"could\",\t\"couldn't\",\t\"did\",\t\"didn't\",\t\"do\",\t\"does\",\t\"doesn't\",\n",
    "                \"doing\",\t\"don't\",\t\"down\",\t\"during\",\t\"each\",\t\"few\",\t\"for\",\t\"from\",\t\"further\",\t\"had\",\t\"hadn't\",\t\"has\",\t\"hasn't\",\t\"have\",\t\"haven't\",\t\"having\",\t\"he\",\n",
    "                \"he'd\",\t\"he'll\",\t\"he's\",\t\"her\",\t\"here\",\t\"here's\",\t\"hers\",\t\"herself\",\t\"him\",\t\"himself\",\t\"his\",\t\"how\",\t\"how's\",\t\"i\",\t\"i'd\",\t\"i'll\",\t\"i'm\",\n",
    "                \"i've\",\t\"if\",\t\"in\",\t\"into\",\t\"is\",\t\"isn't\",\t\"it\",\t\"it's\",\t\"its\",\t\"itself\",\t\"let's\",\t\"me\",\t\"more\",\t\"most\",\t\"mustn't\",\t\"my\",\t\"myself\",\n",
    "                \"no\",\t\"nor\",\t\"not\",\t\"of\",\t\"off\",\t\"on\",\t\"once\",\t\"only\",\t\"or\",\t\"other\",\t\"ought\",\t\"our\",\t\"ours\",\t\"ourselves\",\t\"out\",\t\"over\",\t\"own\",\n",
    "                \"same\",\t\"shan't\",\t\"she\",\t\"she'd\",\t\"she'll\",\t\"she's\",\t\"should\",\t\"shouldn't\",\t\"so\",\t\"some\",\t\"such\",\t\"than\",\t\"that\",\t\"that's\",\t\"the\",\t\"their\",\t\"theirs\",\n",
    "                \"them\",\t\"themselves\",\t\"then\",\t\"there\",\t\"there's\",\t\"these\",\t\"they\",\t\"they'd\",\t\"they'll\",\t\"they're\",\t\"they've\",\t\"this\",\t\"those\",\t\"through\",\t\"to\",\t\"too\",\t\"under\",\n",
    "                \"until\",\t\"up\",\t\"very\",\t\"was\",\t\"wasn't\",\t\"we\",\t\"we'd\",\t\"we'll\",\t\"we're\",\t\"we've\",\t\"were\",\t\"weren't\",\t\"what\",\t\"what's\",\t\"when\",\t\"when's\",\t\"where\",\n",
    "                \"where's\",\t\"which\",\t\"while\",\t\"who\",\t\"who's\",\t\"whom\",\t\"why\",\t\"why's\",\t\"with\",\t\"won't\",\t\"would\",\t\"wouldn't\",\t\"you\",\t\"you'd\",\t\"you'll\",\t\"you're\",\t\"you've\",\n",
    "                \"your\",\t\"yours\",\t\"yourself\",\t\"yourselve\",\t\"a's\",\t\"accordingly\",\t\"again\",\t\"allows\",\t\"also\",\t\"amongst\",\t\"anybody\",\t\"anyways\",\t\"appropriate\",\t\"aside\",\t\"available\",\t\"because\",\t\"before\",\n",
    "                \"below\",\t\"between\",\t\"by\",\t\"can't\",\t\"certain\",\t\"com\",\t\"consider\",\t\"corresponding\",\t\"definitely\",\t\"different\",\t\"don't\",\t\"each\",\t\"else\",\t\"et\",\t\"everybody\",\t\"exactly\",\t\"fifth\",\n",
    "                \"follows\",\t\"four\",\t\"gets\",\t\"goes\",\t\"greetings\",\t\"has\",\t\"he\",\t\"her\",\t\"herein\",\t\"him\",\t\"how\",\t\"i'm\",\t\"immediate\",\t\"indicate\",\t\"instead\",\t\"it\",\t\"itself\",\n",
    "                \"know\",\t\"later\",\t\"lest\",\t\"likely\",\t\"ltd\",\t\"me\",\t\"more\",\t\"must\",\t\"nd\",\t\"needs\",\t\"next\",\t\"none\",\t\"nothing\",\t\"of\",\t\"okay\",\t\"ones\",\t\"others\",\n",
    "                \"ourselves\",\t\"own\",\t\"placed\",\t\"probably\",\t\"rather\",\t\"regarding\",\t\"right\",\t\"saying\",\t\"seeing\",\t\"seen\",\t\"serious\",\t\"she\",\t\"so\",\t\"something\",\t\"soon\",\t\"still\",\t\"t's\",\n",
    "                \"th\",\t\"that\",\t\"theirs\",\t\"there\",\t\"therein\",\t\"they'd\",\t\"third\",\t\"though\",\t\"thus\",\t\"toward\",\t\"try\",\t\"under\",\t\"unto\",\t\"used\",\t\"value\",\t\"vs\",\t\"way\",\n",
    "                \"we've\",\t\"weren't\",\t\"whence\",\t\"whereas\",\t\"whether\",\t\"who's\",\t\"why\",\t\"within\",\t\"wouldn't\",\t\"you'll\",\t\"yourself\",\t\"able\",\t\"across\",\t\"against\",\t\"almost\",\t\"although\",\t\"an\",\n",
    "                \"anyhow\",\t\"anywhere\",\t\"are\",\t\"ask\",\t\"away\",\t\"become\",\t\"beforehand\",\t\"beside\",\t\"beyond\",\t\"c'mon\",\t\"cannot\",\t\"certainly\",\t\"come\",\t\"considering\",\t\"could\",\t\"described\",\t\"do\",\n",
    "                \"done\",\t\"edu\",\t\"elsewhere\",\t\"etc\",\t\"everyone\",\t\"example\",\t\"first\",\t\"for\",\t\"from\",\t\"getting\",\t\"going\",\t\"had\",\t\"hasn't\",\t\"he's\",\t\"here\",\t\"hereupon\",\t\"himself\",\n",
    "                \"howbeit\",\t\"i've\",\t\"in\",\t\"indicated\",\t\"into\",\t\"it'd\",\t\"just\",\t\"known\",\t\"latter\",\t\"let\",\t\"little\",\t\"mainly\",\t\"mean\",\t\"moreover\",\t\"my\",\t\"near\",\t\"neither\",\n",
    "                \"nine\",\t\"noone\",\t\"novel\",\t\"off\",\t\"old\",\t\"only\",\t\"otherwise\",\t\"out\",\t\"particular\",\t\"please\",\t\"provides\",\t\"rd\",\t\"regardless\",\t\"said\",\t\"says\",\t\"seem\",\t\"self\",\n",
    "                \"seriously\",\t\"should\",\t\"some\",\t\"sometime\",\t\"sorry\",\t\"sub\",\t\"take\",\t\"than\",\t\"that's\",\t\"them\",\t\"there's\",\t\"theres\",\t\"they'll\",\t\"this\",\t\"three\",\t\"to\",\t\"towards\",\n",
    "                \"trying\",\t\"unfortunately\",\t\"up\",\t\"useful\",\t\"various\",\t\"want\",\t\"we\",\t\"welcome\",\t\"what\",\t\"whenever\",\t\"whereby\",\t\"which\",\t\"whoever\",\t\"will\",\t\"without\",\t\"yes\",\t\"you're\",\n",
    "                \"yourselves\",\t\"about\",\t\"actually\",\t\"ain't\",\t\"alone\",\t\"always\",\t\"and\",\t\"anyone\",\t\"apart\",\t\"aren't\",\t\"asking\",\t\"awfully\",\t\"becomes\",\t\"behind\",\t\"besides\",\t\"both\",\t\"c's\",\n",
    "                \"cant\",\t\"changes\",\t\"comes\",\t\"contain\",\t\"couldn't\",\t\"despite\",\t\"does\",\t\"down\",\t\"eg\",\t\"enough\",\t\"even\",\t\"everything\",\t\"except\",\t\"five\",\t\"former\",\t\"further\",\t\"given\",\n",
    "                \"gone\",\t\"hadn't\",\t\"have\",\t\"hello\",\t\"here's\",\t\"hers\",\t\"his\",\t\"however\",\t\"ie\",\t\"inasmuch\",\t\"indicates\",\t\"inward\",\t\"it'll\",\t\"keep\",\t\"knows\",\t\"latterly\",\t\"let's\",\n",
    "                \"look\",\t\"many\",\t\"meanwhile\",\t\"most\",\t\"myself\",\t\"nearly\",\t\"never\",\t\"no\",\t\"nor\",\t\"now\",\t\"often\",\t\"on\",\t\"onto\",\t\"ought\",\t\"outside\",\t\"particularly\",\t\"plus\",\n",
    "                \"que\",\t\"re\",\t\"regards\",\t\"same\",\t\"second\",\t\"seemed\",\t\"selves\",\t\"seven\",\t\"shouldn't\",\t\"somebody\",\t\"sometimes\",\t\"specified\",\t\"such\",\t\"taken\",\t\"thank\",\t\"thats\",\t\"themselves\",\n",
    "                \"thereafter\",\t\"thereupon\",\t\"they're\",\t\"thorough\",\t\"through\",\t\"together\",\t\"tried\",\t\"twice\",\t\"unless\",\t\"upon\",\t\"uses\",\t\"very\",\t\"wants\",\t\"we'd\",\t\"well\",\t\"what's\",\t\"where\",\n",
    "                \"wherein\",\t\"while\",\t\"whole\",\t\"willing\",\t\"won't\",\t\"yet\",\t\"you've\",\t\"zero\",\t\"above\",\t\"after\",\t\"all\",\t\"along\",\t\"am\",\t\"another\",\t\"anything\",\t\"appear\",\t\"around\",\n",
    "                \"associated\",\t\"be\",\t\"becoming\",\t\"being\",\t\"best\",\t\"brief\",\t\"came\",\t\"cause\",\t\"clearly\",\t\"concerning\",\t\"containing\",\t\"course\",\t\"did\",\t\"doesn't\",\t\"downwards\",\t\"eight\",\t\"entirely\",\n",
    "                \"ever\",\t\"everywhere\",\t\"far\",\t\"followed\",\t\"formerly\",\t\"furthermore\",\t\"gives\",\t\"got\",\t\"happens\",\t\"haven't\",\t\"help\",\t\"hereafter\",\t\"herself\",\t\"hither\",\t\"i'd\",\t\"if\",\t\"inc\",\n",
    "                \"inner\",\t\"is\",\t\"it's\",\t\"keeps\",\t\"last\",\t\"least\",\t\"like\",\t\"looking\",\t\"may\",\t\"merely\",\t\"mostly\",\t\"name\",\t\"necessary\",\t\"nevertheless\",\t\"nobody\",\t\"normally\",\t\"nowhere\",\n",
    "                \"oh\",\t\"once\",\t\"or\",\t\"our\",\t\"over\",\t\"per\",\t\"possible\",\t\"quite\",\t\"really\",\t\"relatively\",\t\"saw\",\t\"secondly\",\t\"seeming\",\t\"sensible\",\t\"several\",\t\"since\",\t\"somehow\",\n",
    "                \"somewhat\",\t\"specify\",\t\"sup\",\t\"tell\",\t\"thanks\",\t\"the\",\t\"then\",\t\"thereby\",\t\"these\",\t\"they've\",\t\"thoroughly\",\t\"throughout\",\t\"too\",\t\"tries\",\t\"two\",\t\"unlikely\",\t\"us\",\n",
    "                \"using\",\t\"via\",\t\"was\",\t\"we'll\",\t\"went\",\t\"whatever\",\t\"where's\",\t\"whereupon\",\t\"whither\",\t\"whom\",\t\"wish\",\t\"wonder\",\t\"you\",\t\"your\",\t\"according\",\t\"afterwards\",\t\"allow\",\n",
    "                \"already\",\t\"among\",\t\"any\",\t\"anyway\",\t\"appreciate\",\t\"as\",\t\"at\",\t\"became\",\t\"been\",\t\"believe\",\t\"better\",\t\"but\",\t\"can\",\t\"causes\",\t\"co\",\t\"consequently\",\t\"contains\",\n",
    "                \"currently\",\t\"didn't\",\t\"doing\",\t\"during\",\t\"either\",\t\"especially\",\t\"every\",\t\"ex\",\t\"few\",\t\"following\",\t\"forth\",\t\"get\",\t\"go\",\t\"gotten\",\t\"hardly\",\t\"having\",\t\"hence\",\n",
    "                \"hereby\",\t\"hi\",\t\"hopefully\",\t\"i'll\",\t\"ignored\",\t\"indeed\",\t\"insofar\",\t\"isn't\",\t\"its\",\t\"kept\",\t\"lately\",\t\"less\",\t\"liked\",\t\"looks\",\t\"maybe\",\t\"might\",\t\"much\",\n",
    "                \"namely\",\t\"need\",\t\"new\",\t\"non\",\t\"not\",\t\"obviously\",\t\"ok\",\t\"one\",\t\"other\",\t\"ours\",\t\"overall\",\t\"perhaps\",\t\"presumably\",\t\"qv\",\t\"reasonably\",\t\"respectively\",\t\"say\",\n",
    "                \"see\",\t\"seems\",\t\"sent\",\t\"shall\",\t\"six\",\t\"someone\",\t\"somewhere\",\t\"specifying\",\t\"sure\",\t\"tends\",\t\"thanx\",\t\"their\",\t\"thence\",\t\"therefore\",\t\"they\",\t\"think\",\t\"those\",\n",
    "                \"thru\",\t\"took\",\t\"truly\",\t\"un\",\t\"until\",\t\"use\",\t\"usually\",\t\"viz\",\t\"wasn't\",\t\"we're\",\t\"were\",\t\"when\",\t\"whereafter\",\t\"wherever\",\t\"who\",\t\"whose\",\t\"with\",\n",
    "                \"would\",\t\"you'd\",\t\"yours\",\t\"I\",\t\"a\",\t\"about\",\t\"an\",\t\"are\",\t\"as\",\t\"at\",\t\"be\",\t\"by\",\t\"com\",\t\"for\",\t\"from\",\t\"how\",\t\"where\",\n",
    "                \"who\",\t\"will\",\t\"with\",\t\"the\",\t\"www\",\t\"in\",\t\"is\",\t\"it\",\t\"of\",\t\"on\",\t\"or\",\t\"that\",\t\"the\",\t\"this\",\t\"to\",\t\"was\",\t\"what\",\n",
    "                \"when\",\t\"a\",\t\"able\",\t\"about\",\t\"above\",\t\"abst\",\t\"accordance\",\t\"according\",\t\"accordingly\",\t\"across\",\t\"act\",\t\"actually\",\t\"added\",\t\"adj\",\t\"affected\",\t\"affecting\",\t\"affects\",\n",
    "                \"after\",\t\"afterwards\",\t\"again\",\t\"against\",\t\"ah\",\t\"all\",\t\"almost\",\t\"alone\",\t\"along\",\t\"already\",\t\"also\",\t\"although\",\t\"always\",\t\"am\",\t\"among\",\t\"amongst\",\t\"an\",\n",
    "                \"and\",\t\"announce\",\t\"another\",\t\"any\",\t\"anybody\",\t\"anyhow\",\t\"anymore\",\t\"anyone\",\t\"anything\",\t\"anyway\",\t\"anyways\",\t\"anywhere\",\t\"apparently\",\t\"approximately\",\t\"are\",\t\"aren\",\t\"arent\",\n",
    "                \"arise\",\t\"around\",\t\"as\",\t\"aside\",\t\"ask\",\t\"asking\",\t\"at\",\t\"auth\",\t\"available\",\t\"away\",\t\"awfully\",\t\"b\",\t\"back\",\t\"be\",\t\"became\",\t\"because\",\t\"become\",\n",
    "                \"becomes\",\t\"becoming\",\t\"been\",\t\"before\",\t\"beforehand\",\t\"begin\",\t\"beginning\",\t\"beginnings\",\t\"begins\",\t\"behind\",\t\"being\",\t\"believe\",\t\"below\",\t\"beside\",\t\"besides\",\t\"between\",\t\"beyond\",\n",
    "                \"biol\",\t\"both\",\t\"brief\",\t\"briefly\",\t\"but\",\t\"by\",\t\"c\",\t\"ca\",\t\"came\",\t\"can\",\t\"cannot\",\t\"can't\",\t\"cause\",\t\"causes\",\t\"certain\",\t\"certainly\",\t\"co\",\n",
    "                \"com\",\t\"come\",\t\"comes\",\t\"contain\",\t\"containing\",\t\"contains\",\t\"could\",\t\"couldnt\",\t\"d\",\t\"date\",\t\"did\",\t\"didn't\",\t\"different\",\t\"do\",\t\"does\",\t\"doesn't\",\t\"doing\",\n",
    "                \"done\",\t\"don't\",\t\"down\",\t\"downwards\",\t\"due\",\t\"during\",\t\"e\",\t\"each\",\t\"ed\",\t\"edu\",\t\"effect\",\t\"eg\",\t\"eight\",\t\"eighty\",\t\"either\",\t\"else\",\t\"elsewhere\",\n",
    "                \"end\",\t\"ending\",\t\"enough\",\t\"especially\",\t\"et\",\t\"et-al\",\t\"etc\",\t\"even\",\t\"ever\",\t\"every\",\t\"everybody\",\t\"everyone\",\t\"everything\",\t\"everywhere\",\t\"ex\",\t\"except\",\t\"f\",\n",
    "                \"far\",\t\"few\",\t\"ff\",\t\"fifth\",\t\"first\",\t\"five\",\t\"fix\",\t\"followed\",\t\"following\",\t\"follows\",\t\"for\",\t\"former\",\t\"formerly\",\t\"forth\",\t\"found\",\t\"four\",\t\"from\",\n",
    "                \"further\",\t\"furthermore\",\t\"g\",\t\"gave\",\t\"get\",\t\"gets\",\t\"getting\",\t\"give\",\t\"given\",\t\"gives\",\t\"giving\",\t\"go\",\t\"goes\",\t\"gone\",\t\"got\",\t\"gotten\",\t\"h\",\n",
    "                \"had\",\t\"happens\",\t\"hardly\",\t\"has\",\t\"hasn't\",\t\"have\",\t\"haven't\",\t\"having\",\t\"he\",\t\"hed\",\t\"hence\",\t\"her\",\t\"here\",\t\"hereafter\",\t\"hereby\",\t\"herein\",\t\"heres\",\n",
    "                \"hereupon\",\t\"hers\",\t\"herself\",\t\"hes\",\t\"hi\",\t\"hid\",\t\"him\",\t\"himself\",\t\"his\",\t\"hither\",\t\"home\",\t\"how\",\t\"howbeit\",\t\"however\",\t\"hundred\",\t\"i\",\t\"id\",\n",
    "                \"ie\",\t\"if\",\t\"i'll\",\t\"im\",\t\"immediate\",\t\"immediately\",\t\"importance\",\t\"important\",\t\"in\",\t\"inc\",\t\"indeed\",\t\"index\",\t\"information\",\t\"instead\",\t\"into\",\t\"invention\",\t\"inward\",\n",
    "                \"is\",\t\"isn't\",\t\"it\",\t\"itd\",\t\"it'll\",\t\"its\",\t\"itself\",\t\"i've\",\t\"j\",\t\"just\",\t\"k\",\t\"keep\",\t\"keeps\",\t\"kept\",\t\"kg\",\t\"km\",\t\"know\",\n",
    "                \"known\",\t\"knows\",\t\"l\",\t\"largely\",\t\"last\",\t\"lately\",\t\"later\",\t\"latter\",\t\"latterly\",\t\"least\",\t\"less\",\t\"lest\",\t\"let\",\t\"lets\",\t\"like\",\t\"liked\",\t\"likely\",\n",
    "                \"line\",\t\"little\",\t\"'ll\",\t\"look\",\t\"looking\",\t\"looks\",\t\"ltd\",\t\"m\",\t\"made\",\t\"mainly\",\t\"make\",\t\"makes\",\t\"many\",\t\"may\",\t\"maybe\",\t\"me\",\t\"mean\",\n",
    "                \"means\",\t\"meantime\",\t\"meanwhile\",\t\"merely\",\t\"mg\",\t\"might\",\t\"million\",\t\"miss\",\t\"ml\",\t\"more\",\t\"moreover\",\t\"most\",\t\"mostly\",\t\"mr\",\t\"mrs\",\t\"much\",\t\"mug\",\n",
    "                \"must\",\t\"my\",\t\"myself\",\t\"n\",\t\"na\",\t\"name\",\t\"namely\",\t\"nay\",\t\"nd\",\t\"near\",\t\"nearly\",\t\"necessarily\",\t\"necessary\",\t\"need\",\t\"needs\",\t\"neither\",\t\"never\",\n",
    "                \"nevertheless\",\t\"new\",\t\"next\",\t\"nine\",\t\"ninety\",\t\"no\",\t\"nobody\",\t\"non\",\t\"none\",\t\"nonetheless\",\t\"noone\",\t\"nor\",\t\"normally\",\t\"nos\",\t\"not\",\t\"noted\",\t\"nothing\",\n",
    "                \"now\",\t\"nowhere\",\t\"o\",\t\"obtain\",\t\"obtained\",\t\"obviously\",\t\"of\",\t\"off\",\t\"often\",\t\"oh\",\t\"ok\",\t\"okay\",\t\"old\",\t\"omitted\",\t\"on\",\t\"once\",\t\"one\",\n",
    "                \"ones\",\t\"only\",\t\"onto\",\t\"or\",\t\"ord\",\t\"other\",\t\"others\",\t\"otherwise\",\t\"ought\",\t\"our\",\t\"ours\",\t\"ourselves\",\t\"out\",\t\"outside\",\t\"over\",\t\"overall\",\t\"owing\",\n",
    "                \"own\",\t\"p\",\t\"page\",\t\"pages\",\t\"part\",\t\"particular\",\t\"particularly\",\t\"past\",\t\"per\",\t\"perhaps\",\t\"placed\",\t\"please\",\t\"plus\",\t\"poorly\",\t\"possible\",\t\"possibly\",\t\"potentially\",\n",
    "                \"pp\",\t\"predominantly\",\t\"present\",\t\"previously\",\t\"primarily\",\t\"probably\",\t\"promptly\",\t\"proud\",\t\"provides\",\t\"put\",\t\"q\",\t\"que\",\t\"quickly\",\t\"quite\",\t\"qv\",\t\"r\",\t\"ran\",\n",
    "                \"rather\",\t\"rd\",\t\"re\",\t\"readily\",\t\"really\",\t\"recent\",\t\"recently\",\t\"ref\",\t\"refs\",\t\"regarding\",\t\"regardless\",\t\"regards\",\t\"related\",\t\"relatively\",\t\"research\",\t\"respectively\",\t\"resulted\",\n",
    "                \"resulting\",\t\"results\",\t\"right\",\t\"run\",\t\"s\",\t\"said\",\t\"same\",\t\"saw\",\t\"say\",\t\"saying\",\t\"says\",\t\"sec\",\t\"section\",\t\"see\",\t\"seeing\",\t\"seem\",\t\"seemed\",\n",
    "                \"seeming\",\t\"seems\",\t\"seen\",\t\"self\",\t\"selves\",\t\"sent\",\t\"seven\",\t\"several\",\t\"shall\",\t\"she\",\t\"shed\",\t\"she'll\",\t\"shes\",\t\"should\",\t\"shouldn't\",\t\"show\",\t\"showed\",\n",
    "                \"shown\",\t\"showns\",\t\"shows\",\t\"significant\",\t\"significantly\",\t\"similar\",\t\"similarly\",\t\"since\",\t\"six\",\t\"slightly\",\t\"so\",\t\"some\",\t\"somebody\",\t\"somehow\",\t\"someone\",\t\"somethan\",\t\"something\",\n",
    "                \"sometime\",\t\"sometimes\",\t\"somewhat\",\t\"somewhere\",\t\"soon\",\t\"sorry\",\t\"specifically\",\t\"specified\",\t\"specify\",\t\"specifying\",\t\"still\",\t\"stop\",\t\"strongly\",\t\"sub\",\t\"substantially\",\t\"successfully\",\t\"such\",\n",
    "                \"sufficiently\",\t\"suggest\",\t\"sup\",\t\"sure\",\t\"t\",\t\"take\",\t\"taken\",\t\"taking\",\t\"tell\",\t\"tends\",\t\"th\",\t\"than\",\t\"thank\",\t\"thanks\",\t\"thanx\",\t\"that\",\t\"that'll\",\n",
    "                \"thats\",\t\"that've\",\t\"the\",\t\"their\",\t\"theirs\",\t\"them\",\t\"themselves\",\t\"then\",\t\"thence\",\t\"there\",\t\"thereafter\",\t\"thereby\",\t\"thered\",\t\"therefore\",\t\"therein\",\t\"there'll\",\t\"thereof\",\n",
    "                \"therere\",\t\"theres\",\t\"thereto\",\t\"thereupon\",\t\"there've\",\t\"these\",\t\"they\",\t\"theyd\",\t\"they'll\",\t\"theyre\",\t\"they've\",\t\"think\",\t\"this\",\t\"those\",\t\"thou\",\t\"though\",\t\"thoughh\",\n",
    "                \"thousand\",\t\"throug\",\t\"through\",\t\"throughout\",\t\"thru\",\t\"thus\",\t\"til\",\t\"tip\",\t\"to\",\t\"together\",\t\"too\",\t\"took\",\t\"toward\",\t\"towards\",\t\"tried\",\t\"tries\",\t\"truly\",\n",
    "                \"try\",\t\"trying\",\t\"ts\",\t\"twice\",\t\"two\",\t\"u\",\t\"un\",\t\"under\",\t\"unfortunately\",\t\"unless\",\t\"unlike\",\t\"unlikely\",\t\"until\",\t\"unto\",\t\"up\",\t\"upon\",\t\"ups\",\n",
    "                \"us\",\t\"use\",\t\"used\",\t\"useful\",\t\"usefully\",\t\"usefulness\",\t\"uses\",\t\"using\",\t\"usually\",\t\"v\",\t\"value\",\t\"various\",\t\"'ve\",\t\"very\",\t\"via\",\t\"viz\",\t\"vol\",\n",
    "                \"vols\",\t\"vs\",\t\"w\",\t\"want\",\t\"wants\",\t\"was\",\t\"wasnt\",\t\"way\",\t\"we\",\t\"wed\",\t\"welcome\",\t\"we'll\",\t\"went\",\t\"were\",\t\"werent\",\t\"we've\",\t\"what\",\n",
    "                \"whatever\",\t\"what'll\",\t\"whats\",\t\"when\",\t\"whence\",\t\"whenever\",\t\"where\",\t\"whereafter\",\t\"whereas\",\t\"whereby\",\t\"wherein\",\t\"wheres\",\t\"whereupon\",\t\"wherever\",\t\"whether\",\t\"which\",\t\"while\",\n",
    "                \"whim\",\t\"whither\",\t\"who\",\t\"whod\",\t\"whoever\",\t\"whole\",\t\"who'll\",\t\"whom\",\t\"whomever\",\t\"whos\",\t\"whose\",\t\"why\",\t\"widely\",\t\"willing\",\t\"wish\",\t\"with\",\t\"within\",\n",
    "                \"without\",\t\"wont\",\t\"words\",\t\"world\",\t\"would\",\t\"wouldnt\",\t\"www\",\t\"x\",\t\"y\",\t\"yes\",\t\"yet\",\t\"you\",\t\"youd\",\t\"you'll\",\t\"your\",\t\"youre\",\t\"yours\",\n",
    "                \"yourself\",\t\"yourselves\",\"you've\",\"z\",\"zero\",\"initially\",\"you’ll\",\"the\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\n",
    "                \"0\",\"!\",\"£\",\"$\",\"%\",\"^\",\"&\",\"*\",\"(\",\")\",\"_\",\"-\",\"+\",\".\",\",\",\">\",\"<\",\"?\",\"@\",\";\",\";\",\"#\",\"~\"\n",
    "\n",
    "                \n",
    "                \n",
    "                 ]\n",
    "stop.extend(newStopWords)\n",
    "\n",
    "\n",
    "#df['sw_subject'] = df['subject'].str.lower().apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "df['sw_pros'] = df['sw_pros'].str.lower().apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "df['sw_cons'] = df['sw_cons'].str.lower().apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "#df['sw_advice'] = df['advice'].str.lower().apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "\n",
    "\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sw_cons'] = df.apply(lambda row: nltk.word_tokenize(row['sw_cons']), axis=1)\n",
    "df['sw_pros'] = df.apply(lambda row: nltk.word_tokenize(row['sw_pros']), axis=1)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "df['sw_cons'] = df['sw_cons'].apply(lambda x: ' '.join([lemmatizer.lemmatize(w) for w in x] ))\n",
    "df['sw_pros'] = df['sw_pros'].apply(lambda x: ' '.join([lemmatizer.lemmatize(w) for w in x] ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsed word table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_word_stage = df[['unique_id','date', 'star_group', 'sw_cons']]\n",
    "\n",
    "# add comma between each word\n",
    "parsed_word_stage = parsed_word_stage.replace(' ', ',', regex=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>date</th>\n",
       "      <th>star_group</th>\n",
       "      <th>sw_cons</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2019-11-04</td>\n",
       "      <td>Positive</td>\n",
       "      <td>unstable,rota,early,morning,shift</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2020-01-20</td>\n",
       "      <td>Positive</td>\n",
       "      <td>management,lax,food,waste</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2020-01-20</td>\n",
       "      <td>Positive</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2020-01-16</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>management,created,silly,rule,people,working,m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2020-01-15</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>busy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   unique_id       date star_group  \\\n",
       "0          0 2019-11-04   Positive   \n",
       "1          1 2020-01-20   Positive   \n",
       "2          2 2020-01-20   Positive   \n",
       "3          3 2020-01-16    Neutral   \n",
       "4          4 2020-01-15    Neutral   \n",
       "\n",
       "                                             sw_cons  \n",
       "0                  unstable,rota,early,morning,shift  \n",
       "1                          management,lax,food,waste  \n",
       "2                                                     \n",
       "3  management,created,silly,rule,people,working,m...  \n",
       "4                                               busy  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_word_stage.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5200, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sw_cons</th>\n",
       "      <th>date</th>\n",
       "      <th>star_group</th>\n",
       "      <th>unique_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>day</td>\n",
       "      <td>2019-09-13</td>\n",
       "      <td>Positive</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>work</td>\n",
       "      <td>2019-12-11</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kitchen</td>\n",
       "      <td>2019-12-11</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mystery</td>\n",
       "      <td>2019-12-11</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>shopper</td>\n",
       "      <td>2019-12-11</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sw_cons       date star_group  unique_id\n",
       "0      day 2019-09-13   Positive          0\n",
       "1     work 2019-12-11    Neutral          1\n",
       "2  kitchen 2019-12-11    Neutral          1\n",
       "3  mystery 2019-12-11    Neutral          1\n",
       "4  shopper 2019-12-11    Neutral          1"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up empty df\n",
    "master_df = pd.DataFrame()\n",
    "\n",
    "for i, row in parsed_word_stage.iterrows():\n",
    "    \n",
    "    # Create a list of the con words\n",
    "    con_list = row['sw_cons'].split(',')\n",
    "    \n",
    "    # Make df\n",
    "    con_df = pd.DataFrame(con_list, columns = ['sw_cons'])\n",
    "    \n",
    "    # Populate extra columns\n",
    "    con_df['date'] = row['date']\n",
    "    con_df['star_group'] = row['star_group']\n",
    "    con_df['unique_id'] = row['unique_id']\n",
    "  \n",
    "    master_df = master_df.append(con_df, ignore_index = True)\n",
    "    \n",
    "print(master_df.shape)\n",
    "master_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df['post_march_flag'] = 0\n",
    "master_df.loc[master_df.date > '20190301', 'post_march_flag'] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'cons_index.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-58d9c673181e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmaster_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cons_index.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, tupleize_cols, date_format, doublequote, escapechar, decimal)\u001b[0m\n\u001b[0;32m   1743\u001b[0m                                  \u001b[0mdoublequote\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdoublequote\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1744\u001b[0m                                  escapechar=escapechar, decimal=decimal)\n\u001b[1;32m-> 1745\u001b[1;33m         \u001b[0mformatter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1746\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1747\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpath_or_buf\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\formats\\csvs.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    154\u001b[0m             f, handles = _get_handle(self.path_or_buf, self.mode,\n\u001b[0;32m    155\u001b[0m                                      \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 156\u001b[1;33m                                      compression=self.compression)\n\u001b[0m\u001b[0;32m    157\u001b[0m             \u001b[0mclose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36m_get_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text)\u001b[0m\n\u001b[0;32m    398\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m             \u001b[1;31m# Python 3 and encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 400\u001b[1;33m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    401\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mis_text\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    402\u001b[0m             \u001b[1;31m# Python 3 and no explicit encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'cons_index.csv'"
     ]
    }
   ],
   "source": [
    "master_df.to_csv('cons_index.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "pros_parsed_word_stage = df[['unique_id','date', 'star_group', 'sw_pros']]\n",
    "\n",
    "# add comma between each word\n",
    "pros_parsed_word_stage = pros_parsed_word_stage.replace(' ', ',', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4405, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sw_pros</th>\n",
       "      <th>date</th>\n",
       "      <th>star_group</th>\n",
       "      <th>unique_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>training</td>\n",
       "      <td>2019-09-13</td>\n",
       "      <td>Positive</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>starting</td>\n",
       "      <td>2019-09-13</td>\n",
       "      <td>Positive</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>salary</td>\n",
       "      <td>2019-09-13</td>\n",
       "      <td>Positive</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pleasant</td>\n",
       "      <td>2019-09-13</td>\n",
       "      <td>Positive</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>environment</td>\n",
       "      <td>2019-09-13</td>\n",
       "      <td>Positive</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sw_pros       date star_group  unique_id\n",
       "0     training 2019-09-13   Positive          0\n",
       "1     starting 2019-09-13   Positive          0\n",
       "2       salary 2019-09-13   Positive          0\n",
       "3     pleasant 2019-09-13   Positive          0\n",
       "4  environment 2019-09-13   Positive          0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up empty df\n",
    "master_df_pros = pd.DataFrame()\n",
    "\n",
    "for i, row in pros_parsed_word_stage.iterrows():\n",
    "    \n",
    "    # Create a list of the con words\n",
    "    pro_list = row['sw_pros'].split(',')\n",
    "    \n",
    "    # Make df\n",
    "    pro_df = pd.DataFrame(pro_list, columns = ['sw_pros'])\n",
    "    \n",
    "    # Populate extra columns\n",
    "    pro_df['date'] = row['date']\n",
    "    pro_df['star_group'] = row['star_group']\n",
    "    pro_df['unique_id'] = row['unique_id']\n",
    "  \n",
    "    master_df_pros = master_df_pros.append(pro_df, ignore_index = True)\n",
    "    \n",
    "print(master_df_pros.shape)\n",
    "master_df_pros.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df_pros['post_march_flag'] = 0\n",
    "master_df_pros.loc[master_df_pros.date > '20190301', 'post_march_flag'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df_pros.to_csv('pros_index.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "import ast\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def phrase_counter(df, col, word_range, additional_stop_words = [], output_number = 100):\n",
    "\n",
    "    # Change keyword argument\n",
    "    stop_status = ENGLISH_STOP_WORDS.union(additional_stop_words)\n",
    "\n",
    "    # Initiate the vectoriser\n",
    "\n",
    "    # Use kwargs min_word_range, max_word_range and reference stop words\n",
    "    vect = CountVectorizer(ngram_range=(word_range, word_range), stop_words=stop_status)\n",
    "\n",
    "    # Fill with balnk spaces\n",
    "    df[col] = df[col].fillna('')\n",
    "\n",
    "    # Drop any numbers\n",
    "\n",
    "\n",
    "    # Just into one big string\n",
    "    summaries = ''.join(df[col])\n",
    "    ngram_summaries = vect.build_analyzer()(summaries)\n",
    "\n",
    "    # Now generate output - use output number here\n",
    "    count_list = Counter(ngram_summaries).most_common(output_number)\n",
    "\n",
    "    wordcount_dict = {'word' : [],\n",
    "                     'count' : []}\n",
    "\n",
    "    # Loop through the count_list\n",
    "    for element in count_list:\n",
    "\n",
    "        # Append the phrases and counts to dict\n",
    "        wordcount_dict['word'].append(element[0])\n",
    "        wordcount_dict['count'].append(element[1])\n",
    "\n",
    "    # Output a dataframe\n",
    "    count_df = pd.DataFrame(wordcount_dict)\n",
    "\n",
    "    # Just so look pretty\n",
    "    count_df = count_df[['word', 'count']]\n",
    "\n",
    "    return count_df\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>team member</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>team leader</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hard work</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>working hour</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>early morning</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>long hour</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>low pay</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>work pret</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>pret manger</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>work hard</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>work work</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>hour shift</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>key role</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>paced environment</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>fast paced</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>upper management</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>poor management</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>staff member</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>depending shop</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>manager leader</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>work time</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>pret standard</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>member manager</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>work pay</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>shopper bonus</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>higher position</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>bad pay</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>hard working</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>work longer</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>hot food</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>money bad</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>broken promise</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>cut hour</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>job stressful</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>worth money</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>bad company</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>stay longer</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>extra hour</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>kitchen leader</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>hour stressful</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>pressure deliver</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>lunch time</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>manager team</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>work early</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>shop manager</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>stress constant</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>lack staff</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>care feel</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>place hell</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>work management</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>depends manager</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>shift expect</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>pay stress</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>hour start</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>environment poor</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>work terrible</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>manager shouting</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>waste time</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>staff shop</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>work free</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 word  count\n",
       "0         team member     24\n",
       "1         team leader     11\n",
       "2           hard work      8\n",
       "3        working hour      7\n",
       "4       early morning      7\n",
       "5           long hour      7\n",
       "6             low pay      6\n",
       "7           work pret      6\n",
       "8         pret manger      5\n",
       "9           work hard      5\n",
       "10          work work      5\n",
       "11         hour shift      5\n",
       "12           key role      5\n",
       "13  paced environment      5\n",
       "14         fast paced      5\n",
       "15   upper management      5\n",
       "16    poor management      4\n",
       "17       staff member      4\n",
       "18     depending shop      4\n",
       "19     manager leader      4\n",
       "20          work time      4\n",
       "21      pret standard      4\n",
       "22     member manager      4\n",
       "23           work pay      4\n",
       "24      shopper bonus      3\n",
       "25    higher position      3\n",
       "26            bad pay      3\n",
       "27       hard working      3\n",
       "28        work longer      3\n",
       "29           hot food      3\n",
       "..                ...    ...\n",
       "70          money bad      2\n",
       "71     broken promise      2\n",
       "72           cut hour      2\n",
       "73      job stressful      2\n",
       "74        worth money      2\n",
       "75        bad company      2\n",
       "76        stay longer      2\n",
       "77         extra hour      2\n",
       "78     kitchen leader      2\n",
       "79     hour stressful      2\n",
       "80   pressure deliver      2\n",
       "81         lunch time      2\n",
       "82       manager team      2\n",
       "83         work early      2\n",
       "84       shop manager      2\n",
       "85    stress constant      2\n",
       "86         lack staff      2\n",
       "87          care feel      2\n",
       "88         place hell      2\n",
       "89    work management      2\n",
       "90    depends manager      2\n",
       "91       shift expect      2\n",
       "92         pay stress      2\n",
       "93         hour start      2\n",
       "94   environment poor      2\n",
       "95      work terrible      2\n",
       "96   manager shouting      2\n",
       "97         waste time      2\n",
       "98         staff shop      2\n",
       "99          work free      2\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase_counter(df, 'sw_cons', 2, additional_stop_words=['mystery'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
